# ğŸ¥ **Cardio Prediction Flask App**  

## ğŸš‘ **Cardiovascular Disease Prediction**  

## ğŸ“Š **Project Overview**  
This project aims to **predict the risk of cardiovascular disease** using **machine learning techniques**. The dataset, sourced from **Kaggle**, contains **70,000 samples** with **13 features**. The solution involves **data preprocessing, EDA, feature engineering, and classification modeling** to achieve high accuracy.  

---

## ğŸ“ **Dataset**  
- **ğŸ“Œ Source:** Cardiovascular Disease Detection - Kaggle  
- **ğŸ“ Shape:** (70000, 13)  
- **ğŸ¯ Target Variable:** Cardiovascular Disease (**Binary Classification - 0 or 1**)  

---

## ğŸ”§ **Project Workflow**  

### ğŸ› ï¸ 1. Data Preprocessing  
âœ… Handled **missing values, duplicates, and outliers**  
âœ… Performed **data cleaning and normalization**  
âœ… **SMOTE** applied to balance the dataset  

### ğŸ“Š 2. Exploratory Data Analysis (EDA)  
ğŸ“Œ **Pairplots** to visualize feature relationships  
ğŸ“Œ Identified **feature distributions** and **correlations**  
ğŸ“Œ Split data into **X (features) & y (target)**  

### ğŸ” 3. Feature Engineering  
ğŸ“Œ **Converted age** from days to years  
ğŸ“Œ Created **BMI (Body Mass Index)** column  
ğŸ“Œ Applied **StandardScaler** for feature scaling  

### ğŸ¤– 4. Machine Learning Models  
Implemented multiple models for **risk prediction**:  
ğŸ”¹ **Logistic Regression**  
ğŸ”¹ **Gradient Boosting Classifier**  
ğŸ”¹ **Random Forest Classifier**  
ğŸ”¹ **Decision Tree Classifier**  
ğŸ”¹ **XGBoost Classifier**  

---

## ğŸ“ˆ **Results**  

### **ğŸ“Š Model Comparison: Before Feature Engineering & Scaling**  

| ğŸ† Model | ğŸ¯ Precision | ğŸ”„ Recall | ğŸ“Š F1-Score | âœ… Accuracy |
|---------|------------|---------|-----------|-----------|
| ğŸ”¹ Logistic Regression | 0.70 | 0.70 | 0.70 | 70% |
| ğŸš€ Gradient Boosting | 0.74 | 0.74 | 0.74 | 74% |
| ğŸŒ³ Random Forest | 0.72 | 0.72 | 0.72 | 72% |
| ğŸŒ± Decision Tree | 0.63 | 0.63 | 0.63 | 63% |
| âš¡ XGBoost | 0.74 | 0.74 | 0.74 | 74% |

### **ğŸ“Š Model Comparison: After Feature Engineering & Scaling**  

| ğŸ† Model | ğŸ¯ Precision | ğŸ”„ Recall | ğŸ“Š F1-Score | âœ… Accuracy |
|---------|------------|---------|-----------|-----------|
| ğŸ”¹ Logistic Regression | 0.73 | 0.72 | 0.72 | 72% |
| ğŸš€ Gradient Boosting | 0.74 | 0.74 | 0.74 | 74% |
| ğŸŒ³ Random Forest | 0.72 | 0.72 | 0.72 | 72% |
| ğŸŒ± Decision Tree | 0.64 | 0.64 | 0.64 | 64% |
| âš¡ XGBoost | 0.74 | 0.74 | 0.74 | 74% |

### **ğŸ† Final Model: XGBoost Classifier**  
After **hyperparameter tuning**, **XGBoost** achieved the best performance:  
âœ… **Test Set Accuracy:** **74%**  
âœ… **Precision:** **0.75**  
âœ… **Recall:** **0.74**  
âœ… **F1-Score:** **0.74**  

---

## ğŸ§  **Key Insights**  
ğŸ“Œ **Feature Engineering** (age conversion & BMI addition) **boosted model performance**  
ğŸ“Œ **XGBoost Classifier outperformed all models** with **74% accuracy**  
ğŸ“Œ **SMOTE** helped balance the dataset and improve prediction for both classes  

---

## ğŸ› ï¸ **Technologies Used**  
ğŸ”¹ **Python** â€“ Data preprocessing & modeling  
ğŸ”¹ **Pandas, NumPy** â€“ Data manipulation  
ğŸ”¹ **Scikit-Learn** â€“ ML model implementation  
ğŸ”¹ **XGBoost** â€“ Final model optimization  
ğŸ”¹ **Flask** â€“ Web app for deployment  
ğŸ”¹ **HTML/CSS** â€“ Web interface design  

---

## ğŸš€ **Future Improvements**  
ğŸ”¹ **Deep Learning** â€“ Implement **Neural Networks** (e.g., LSTMs, CNNs)  
ğŸ”¹ **Real-Time Predictions** â€“ Deploy the model with an **API**  
ğŸ”¹ **Hyperparameter Optimization** â€“ Further **fine-tune models**  

---

## Flast App Look
![flask_app_page](https://github.com/user-attachments/assets/38d49885-5e2b-4497-8b23-60ab6e100f17)

---

## ğŸ™‹â€â™‚ï¸ **Author**  
ğŸ‘¤ **Bipin Waghmare**  
ğŸ’» **Data Scientist & ML Enthusiast**  
ğŸ”— **[GitHub](#) | [LinkedIn](#)**  

---

## ğŸ“ **License**  
This project is licensed under the **MIT License**.  

---

## ğŸ– **Acknowledgments**  
Special thanks to **Kaggle & open-source contributors** for providing the dataset.  
